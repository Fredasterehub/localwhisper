--- a/core/settings.py
+++ b/core/settings.py
@@ -1,6 +1,7 @@
 import json
 import os
 import config
+from core.logger import log
 
 class SettingsManager:
     def __init__(self):
@@ -13,9 +14,126 @@
             "input_device_index": None, # Default device
             "use_intelligence": False, # Default to Raw Mode (User Preference)
             "transcription_language": "auto", # auto, en, fr
+
+            # Voice-activated mode (Discord-like) tuning
+            # Start gate = Silero speech probability + adaptive noise floor (RMS dBFS margin)
+            "voice_activation_start_confirm_ms": 220,
+            "voice_activation_hangover_ms": 160,
+            "voice_activation_cooldown_ms": 350,
+            "voice_activation_pre_roll_ms": 550,
+            "voice_activation_min_segment_ms": 450,
+            "voice_activation_min_speech_ms": 220,
+            "voice_activation_max_segment_s": 60.0,
+            "voice_activation_start_speech_prob": 0.62,
+            "voice_activation_stop_speech_prob": 0.45,
+            "voice_activation_start_db_margin": 8.0,
+            "voice_activation_stop_db_margin": 4.0,
+            "voice_activation_noise_update_speech_prob": 0.20,
+            "voice_activation_noise_ema_alpha": 0.04,
+
+            # Transcription stability (faster-whisper decode options)
+            "decode_beam_size": 8,
+            "decode_temperature": 0.0,
+            "decode_best_of": 1,
+            "decode_patience": 1.0,
+            "decode_length_penalty": 1.0,
+            "decode_repetition_penalty": 1.08,
+            "decode_no_repeat_ngram_size": 0,
+            "decode_condition_on_previous_text": True,
+            "decode_no_speech_threshold": 0.6,
+            "decode_log_prob_threshold": -1.0,
+            "decode_compression_ratio_threshold": 2.35,
+            "language_detection_threshold": 0.6,
+            "language_detection_segments": 3,
+
+            # Noisy-audio second pass (quality-first, may add ~0.2-1.0s)
+            "decode_enable_noisy_second_pass": True,
+            "decode_noisy_beam_size": 10,
+            "decode_noisy_best_of": 1,
+            "decode_noisy_condition_on_previous_text": False,
+
+            # Sticky language (prevents FR/EN flip-flop when language is set to "auto")
+            "sticky_language_enabled": True,
+            "sticky_language_min_prob": 0.90,
+            "sticky_language_ttl_s": 180.0,
+            "sticky_language_redetect_interval_s": 60.0,
+
+            # Auto language disambiguation (FR/EN): only triggers extra decode when detection is ambiguous.
+            "auto_languages": ["en", "fr"],
+            "auto_language_ambiguity_min_prob": 0.88,
+            "auto_language_ambiguity_min_margin": 0.12,
+            "auto_language_force_on_short_utterance": True,
+            "auto_language_short_utterance_s": 2.5,
+
+            # Confidence heuristics (reject likely hallucinations / noise-only captures)
+            "reject_no_speech_prob": 0.85,
+            "reject_avg_logprob": -0.95,
+            "reject_min_chars": 2,
+
+            # Confidence classification thresholds (used to decide retry/LLM/reject)
+            "conf_high_min_avg_logprob": -0.55,
+            "conf_high_max_avg_no_speech_prob": 0.55,
+            "conf_high_max_avg_compression_ratio": 2.05,
+            "conf_med_min_avg_logprob": -0.85,
+            "conf_med_max_avg_no_speech_prob": 0.78,
+            "conf_med_max_avg_compression_ratio": 2.35,
+
+            # Injection behavior
+            "inject_typing_max_chars": 32,
+            "inject_terminal_always_paste": True,
+            "inject_clipboard_settle_ms": 80,
+            "inject_clipboard_restore_delay_ms": 550,
+            "inject_clipboard_retry_count": 6,
+            "inject_clipboard_retry_backoff_ms": 20,
+            "terminal_processes": [
+                "windowsterminal.exe",
+                "wt.exe",
+                "conhost.exe",
+                "cmd.exe",
+                "powershell.exe",
+                "pwsh.exe",
+                "openconsole.exe",
+                "wezterm.exe",
+                "alacritty.exe",
+                "mintty.exe",
+                "tabby.exe",
+                "hyper.exe",
+                "putty.exe",
+            ],
+            "paste_hotkey_order": ["ctrl+shift+v", "shift+insert", "ctrl+v"],
+
+            # LLM refinement safety (Ollama)
+            "ollama_timeout_s": 6.0,
+            "llm_refine_max_chars": 420,
+            "llm_refine_skip_code_like": True,
+            "llm_refine_min_confidence": "high",  # high|medium|low
+            "llm_refine_min_audio_s": 2.5,
+            "llm_refine_min_words": 6,
+
+            # Voice activation debug (logs segment summaries)
+            "voice_activation_debug": False,
+
+            # UI timing
+            "success_hold_ms": 350,
+
+            # Overlay skin
+            "overlay_skin": "matrix_rain",  # matrix_rain|dot|sauron_eye
+
+            # Sauron eye skin tuning
+            "sauron_fire_fps_ms": 40,
             "setup_completed": False
         }
         self.settings = self.load_settings()
+
+    def _warn_and_prune_unknown_keys(self, raw: dict) -> dict:
+        if not isinstance(raw, dict):
+            return {}
+
+        known = set(self.defaults.keys())
+        unknown = sorted([k for k in raw.keys() if k not in known])
+        if unknown:
+            log(f"Unknown settings keys ignored: {unknown}", "warning")
+        return {k: v for k, v in raw.items() if k in known}
 
     def load_settings(self):
         if not os.path.exists(self.settings_path):
@@ -24,6 +142,7 @@
         try:
             with open(self.settings_path, 'r') as f:
                 data = json.load(f)
+                data = self._warn_and_prune_unknown_keys(data)
                 # Merge with defaults to ensure all keys exist
                 merged = self.defaults.copy()
                 merged.update(data)
@@ -44,6 +163,9 @@
         return self.settings.get(key, self.defaults.get(key))
 
     def set(self, key, value):
+        if key not in self.defaults:
+            log(f"Attempt to set unknown setting ignored: {key}", "warning")
+            return
         self.settings[key] = value
         self.save_settings()
 
--- a/core/transcriber.py
+++ b/core/transcriber.py
@@ -2,18 +2,252 @@
 import config
 import os
 import time
+import inspect
+from core.settings import manager as settings
+from core.logger import log
 
 class Transcriber:
     def __init__(self):
         print(f"Loading Whisper Model: {config.WHISPER_MODEL_SIZE} on {config.DEVICE}...")
         start = time.time()
-        self.model = WhisperModel(
-            config.WHISPER_MODEL_SIZE, 
-            device=config.DEVICE, 
-            compute_type=config.COMPUTE_TYPE,
-            download_root=config.MODELS_DIR
+        self._compute_type_effective = config.COMPUTE_TYPE
+        try:
+            self.model = WhisperModel(
+                config.WHISPER_MODEL_SIZE,
+                device=config.DEVICE,
+                compute_type=config.COMPUTE_TYPE,
+                download_root=config.MODELS_DIR,
+            )
+        except Exception as e:
+            # Explicit fallback for reliability (VRAM/driver issues): retry with int8.
+            log(f"Whisper model load failed with compute_type={config.COMPUTE_TYPE}: {e}", "warning")
+            self._compute_type_effective = "int8"
+            self.model = WhisperModel(
+                config.WHISPER_MODEL_SIZE,
+                device=config.DEVICE,
+                compute_type="int8",
+                download_root=config.MODELS_DIR,
+            )
+        print(f"Model loaded in {time.time() - start:.2f}s (compute_type={self._compute_type_effective})")
+
+        self._sticky_language = None
+        self._sticky_set_time = 0.0
+        self._last_redetect_time = 0.0
+        self._warned_unsupported_args = set()
+        self._transcribe_sig = inspect.signature(WhisperModel.transcribe)
+
+        # Last-result metadata for downstream policy (LLM/refuse/etc.)
+        self.last_confidence = "unknown"  # high|medium|low|silence|unknown
+        self.last_stats = {}
+
+    def _choose_language(self, requested_language: str | None) -> str | None:
+        if requested_language:
+            return requested_language
+
+        if settings.get("transcription_language") != "auto":
+            return settings.get("transcription_language")
+
+        # Auto mode: let Whisper detect language per utterance.
+        return None
+
+    def _get_auto_languages(self) -> list[str]:
+        langs = settings.get("auto_languages")
+        if isinstance(langs, list) and langs:
+            out = []
+            for x in langs:
+                s = str(x).strip().lower()
+                if s and s not in out:
+                    out.append(s)
+            if out:
+                return out
+        return ["en", "fr"]
+
+    @staticmethod
+    def _sorted_language_probs(info) -> list[tuple[str, float]]:
+        try:
+            probs = getattr(info, "all_language_probs", None)
+            if probs:
+                return sorted([(str(l).lower(), float(p)) for (l, p) in probs], key=lambda x: x[1], reverse=True)
+        except Exception:
+            pass
+        try:
+            return [(str(getattr(info, "language", "")).lower(), float(getattr(info, "language_probability", 0.0)))]
+        except Exception:
+            return []
+
+    def _is_language_ambiguous(self, info) -> bool:
+        """
+        Treat language as ambiguous when detection is weak or close.
+        This is the main driver for the extra FR/EN forced-decode pass.
+        """
+        probs = self._sorted_language_probs(info)
+        if not probs:
+            return True
+
+        top_lang, top_p = probs[0]
+        second_p = probs[1][1] if len(probs) > 1 else 0.0
+
+        min_prob = float(settings.get("auto_language_ambiguity_min_prob"))
+        min_margin = float(settings.get("auto_language_ambiguity_min_margin"))
+        auto_langs = set(self._get_auto_languages())
+
+        if top_lang not in auto_langs:
+            return True
+        if top_p < min_prob:
+            return True
+        if (top_p - second_p) < min_margin:
+            return True
+        return False
+
+    def _maybe_update_sticky_language(self, info, audio_seconds: float):
+        if not settings.get("sticky_language_enabled"):
+            return
+
+        lang = str(getattr(info, "language", "") or "").lower()
+        prob = float(getattr(info, "language_probability", 0.0))
+        min_prob = float(settings.get("sticky_language_min_prob"))
+        ttl_s = float(settings.get("sticky_language_ttl_s"))
+        now = time.time()
+
+        if not lang:
+            return
+
+        # For very short utterances, avoid "locking in" on a potentially noisy detection.
+        if audio_seconds < 1.0:
+            return
+
+        if prob >= min_prob:
+            self._sticky_language = lang
+            self._sticky_set_time = now
+            return
+
+        # If sticky expired, allow updates at lower confidence to recover.
+        if self._sticky_language and (now - self._sticky_set_time) > ttl_s:
+            if prob >= (min_prob - 0.08):
+                self._sticky_language = lang
+                self._sticky_set_time = now
+
+    def _validate_and_build_decode_args(self, noisy: bool) -> dict:
+        """
+        Build a dict of WhisperModel.transcribe kwargs from Settings.
+        Enforces: unknown/unsupported args are not silently applied.
+        """
+        def clamp_int(name: str, value, lo: int, hi: int, default: int) -> int:
+            try:
+                v = int(value)
+            except Exception:
+                log(f"Invalid {name}={value!r}; using {default}.", "warning")
+                return default
+            if v < lo or v > hi:
+                log(f"Out-of-range {name}={v}; clamping to [{lo},{hi}].", "warning")
+                v = max(lo, min(hi, v))
+            return v
+
+        def clamp_float(name: str, value, lo: float, hi: float, default: float) -> float:
+            try:
+                v = float(value)
+            except Exception:
+                log(f"Invalid {name}={value!r}; using {default}.", "warning")
+                return default
+            if v < lo or v > hi:
+                log(f"Out-of-range {name}={v}; clamping to [{lo},{hi}].", "warning")
+                v = max(lo, min(hi, v))
+            return float(v)
+
+        if noisy:
+            beam_size = clamp_int("decode_noisy_beam_size", settings.get("decode_noisy_beam_size"), 1, 20, 10)
+            best_of = clamp_int("decode_noisy_best_of", settings.get("decode_noisy_best_of"), 1, 10, 1)
+            condition_on_previous_text = bool(settings.get("decode_noisy_condition_on_previous_text"))
+        else:
+            beam_size = clamp_int("decode_beam_size", settings.get("decode_beam_size"), 1, 20, 8)
+            best_of = clamp_int("decode_best_of", settings.get("decode_best_of"), 1, 10, 1)
+            condition_on_previous_text = bool(settings.get("decode_condition_on_previous_text"))
+
+        temperature = clamp_float("decode_temperature", settings.get("decode_temperature"), 0.0, 1.0, 0.0)
+        patience = clamp_float("decode_patience", settings.get("decode_patience"), 0.1, 2.5, 1.0)
+        length_penalty = clamp_float("decode_length_penalty", settings.get("decode_length_penalty"), 0.5, 1.5, 1.0)
+        repetition_penalty = clamp_float("decode_repetition_penalty", settings.get("decode_repetition_penalty"), 1.0, 1.5, 1.08)
+        no_repeat_ngram_size = clamp_int("decode_no_repeat_ngram_size", settings.get("decode_no_repeat_ngram_size"), 0, 10, 0)
+        no_speech_threshold = clamp_float("decode_no_speech_threshold", settings.get("decode_no_speech_threshold"), 0.0, 1.0, 0.6)
+        log_prob_threshold = clamp_float("decode_log_prob_threshold", settings.get("decode_log_prob_threshold"), -5.0, 0.0, -1.0)
+        compression_ratio_threshold = clamp_float("decode_compression_ratio_threshold", settings.get("decode_compression_ratio_threshold"), 1.5, 3.5, 2.35)
+        language_detection_threshold = clamp_float("language_detection_threshold", settings.get("language_detection_threshold"), 0.0, 1.0, 0.6)
+        language_detection_segments = clamp_int("language_detection_segments", settings.get("language_detection_segments"), 1, 10, 2)
+
+        # Preferred args set (hardcoded whitelist) -> strict validation vs installed faster-whisper.
+        desired = {
+            "beam_size": beam_size,
+            "best_of": best_of,
+            "patience": patience,
+            "length_penalty": length_penalty,
+            "repetition_penalty": repetition_penalty,
+            "no_repeat_ngram_size": no_repeat_ngram_size,
+            "temperature": temperature,
+            "condition_on_previous_text": condition_on_previous_text,
+            "no_speech_threshold": no_speech_threshold,
+            "log_prob_threshold": log_prob_threshold,
+            "compression_ratio_threshold": compression_ratio_threshold,
+            "language_detection_threshold": language_detection_threshold,
+            "language_detection_segments": language_detection_segments,
+
+            # Always fixed for this app (audio already segmented by VAD).
+            "vad_filter": False,
+            "word_timestamps": False,
+        }
+
+        supported = set(self._transcribe_sig.parameters.keys())
+        effective = {}
+        unsupported = sorted([k for k in desired.keys() if k not in supported])
+        if unsupported:
+            for k in unsupported:
+                if k not in self._warned_unsupported_args:
+                    log(f"Unsupported faster-whisper transcribe arg ignored: {k}", "warning")
+                    self._warned_unsupported_args.add(k)
+
+        for k, v in desired.items():
+            if k in supported:
+                effective[k] = v
+        return effective
+
+    def _classify_confidence(self, segments: list, text: str) -> tuple[str, dict]:
+        if not segments or not text:
+            return "silence", {"reason": "empty"}
+
+        avg_no_speech = sum(float(s.no_speech_prob) for s in segments) / max(1, len(segments))
+        avg_logprob = sum(float(s.avg_logprob) for s in segments) / max(1, len(segments))
+        avg_compression = sum(float(s.compression_ratio) for s in segments) / max(1, len(segments))
+
+        stats = {
+            "avg_no_speech_prob": float(avg_no_speech),
+            "avg_logprob": float(avg_logprob),
+            "avg_compression_ratio": float(avg_compression),
+            "segments": int(len(segments)),
+            "chars": int(len(text)),
+        }
+
+        # Hard reject: very likely silence + low logprob.
+        reject_no_speech_prob = float(settings.get("reject_no_speech_prob"))
+        reject_avg_logprob = float(settings.get("reject_avg_logprob"))
+        if avg_no_speech >= reject_no_speech_prob and avg_logprob <= reject_avg_logprob:
+            return "silence", {**stats, "reason": "no_speech"}
+
+        high = (
+            avg_logprob >= float(settings.get("conf_high_min_avg_logprob"))
+            and avg_no_speech <= float(settings.get("conf_high_max_avg_no_speech_prob"))
+            and avg_compression <= float(settings.get("conf_high_max_avg_compression_ratio"))
         )
-        print(f"Model loaded in {time.time() - start:.2f}s")
+        if high:
+            return "high", stats
+
+        medium = (
+            avg_logprob >= float(settings.get("conf_med_min_avg_logprob"))
+            and avg_no_speech <= float(settings.get("conf_med_max_avg_no_speech_prob"))
+            and avg_compression <= float(settings.get("conf_med_max_avg_compression_ratio"))
+        )
+        if medium:
+            return "medium", stats
+
+        return "low", stats
 
     def transcribe(self, audio_data, language=None):
         """
@@ -23,12 +257,124 @@
         if audio_data.dtype != "float32":
             audio_data = audio_data.astype("float32")
 
-        # Use passed language (or None for auto)
-        segments,info = self.model.transcribe(audio_data, beam_size=5, language=language, task="transcribe")
-        
-        # Collect all text
-        text = " ".join([segment.text for segment in segments])
-        return text.strip()
+        chosen_language = self._choose_language(language)
+
+        base_args = self._validate_and_build_decode_args(noisy=False)
+        segments, info = self.model.transcribe(
+            audio_data,
+            task="transcribe",
+            language=chosen_language,
+            **base_args,
+        )
+
+        segments = list(segments)
+        text = " ".join([segment.text for segment in segments]).strip()
+
+        confidence, stats = self._classify_confidence(segments, text)
+        audio_seconds = float(len(audio_data) / float(getattr(config, "SAMPLE_RATE", 16000))) if hasattr(audio_data, "__len__") else 0.0
+        stats = {**stats, "audio_seconds": audio_seconds}
+        self.last_confidence = confidence
+        self.last_stats = stats
+
+        min_chars = int(settings.get("reject_min_chars"))
+        if len(text) < min_chars:
+            self.last_confidence = "silence"
+            self.last_stats = {**stats, "reason": "too_short"}
+            return ""
+
+        # Optional quality-first second pass when the first decode looks noisy.
+        if confidence == "low" and settings.get("decode_enable_noisy_second_pass"):
+            noisy_args = self._validate_and_build_decode_args(noisy=True)
+            segments2, info2 = self.model.transcribe(
+                audio_data,
+                task="transcribe",
+                language=chosen_language,
+                **noisy_args,
+            )
+            segments2 = list(segments2)
+            text2 = " ".join([s.text for s in segments2]).strip()
+            conf2, stats2 = self._classify_confidence(segments2, text2)
+
+            # Choose the higher-confidence result; tie-breaker by avg_logprob.
+            rank = {"high": 3, "medium": 2, "low": 1, "silence": 0, "unknown": 0}
+            choose_second = False
+            if rank.get(conf2, 0) > rank.get(confidence, 0):
+                choose_second = True
+            elif conf2 == confidence:
+                if stats2.get("avg_logprob", -9) > stats.get("avg_logprob", -9):
+                    choose_second = True
+
+            if choose_second:
+                segments, text, confidence, stats = segments2, text2, conf2, stats2
+                info = info2
+                self.last_confidence = confidence
+                self.last_stats = {**stats, "audio_seconds": audio_seconds, "pass": "noisy_second"}
+            else:
+                self.last_stats = {**stats, "audio_seconds": audio_seconds, "pass": "base"}
+
+        if self.last_confidence in {"silence", "low"}:
+            # Quality > latency: do not inject low-confidence text.
+            return ""
+
+        # Auto language stabilization (FR/EN): if language detection is ambiguous, decode again with forced languages
+        # and choose the one with better confidence.
+        if chosen_language is None and settings.get("transcription_language") == "auto":
+            force_on_short = bool(settings.get("auto_language_force_on_short_utterance"))
+            short_s = float(settings.get("auto_language_short_utterance_s"))
+            short_utterance = force_on_short and (audio_seconds > 0.0) and (audio_seconds <= short_s)
+
+            # Always disambiguate if the detected language is not in our allowed auto set.
+            top_lang = str(getattr(info, "language", "") or "").lower()
+            auto_langs = set(self._get_auto_languages())
+            not_allowed_lang = bool(top_lang) and (top_lang not in auto_langs)
+
+            if short_utterance or not_allowed_lang or self._is_language_ambiguous(info):
+                auto_langs = self._get_auto_languages()
+                # Prefer sticky language first (helps prevent random flips on short utterances).
+                if self._sticky_language and self._sticky_language in auto_langs:
+                    ordered = [self._sticky_language] + [l for l in auto_langs if l != self._sticky_language]
+                else:
+                    ordered = auto_langs
+
+                # Use the same args as the selected pass (base/noisy).
+                args = noisy_args if self.last_stats.get("pass") == "noisy_second" else base_args
+                best = ({"high": 3, "medium": 2, "low": 1, "silence": 0, "unknown": 0}.get(self.last_confidence, 0), stats.get("avg_logprob", -9.0), text, segments, info, getattr(info, "language", None))
+
+                for lang in ordered[:2]:
+                    seg_l, info_l = self.model.transcribe(
+                        audio_data,
+                        task="transcribe",
+                        language=lang,
+                        **args,
+                    )
+                    seg_l = list(seg_l)
+                    text_l = " ".join([s.text for s in seg_l]).strip()
+                    conf_l, stats_l = self._classify_confidence(seg_l, text_l)
+                    stats_l = {**stats_l, "audio_seconds": audio_seconds}
+                    score = ({"high": 3, "medium": 2, "low": 1, "silence": 0, "unknown": 0}.get(conf_l, 0), stats_l.get("avg_logprob", -9.0), text_l, seg_l, info_l, lang)
+                    if score[0] > best[0] or (score[0] == best[0] and score[1] > best[1]):
+                        best = score
+
+                # Apply best candidate
+                best_conf_rank, best_logprob, best_text, best_segments, best_info, best_lang = best
+                self.last_confidence, self.last_stats = self._classify_confidence(best_segments, best_text)
+                self.last_stats = {**self.last_stats, "audio_seconds": audio_seconds, "pass": "auto_lang_disambiguated", "lang": best_lang}
+                text, segments, info = best_text, best_segments, best_info
+
+            self._maybe_update_sticky_language(info, audio_seconds)
+
+        return text
+
+    def dump_effective_decode_args(self) -> dict:
+        """
+        Returns the actual kwargs we will pass to WhisperModel.transcribe for both base/noisy.
+        Useful for debugging and for detecting API mismatches.
+        """
+        return {
+            "base": self._validate_and_build_decode_args(noisy=False),
+            "noisy": self._validate_and_build_decode_args(noisy=True),
+            "compute_type_effective": self._compute_type_effective,
+        }
 
 if __name__ == "__main__":
     t = Transcriber()
--- a/main.py
+++ b/main.py
@@ -127,6 +127,38 @@
 
     processing_lock = threading.Lock()
     stop_processing_flag = False
+
+    def get_success_hold_s() -> float:
+        try:
+            return max(0.05, float(settings.get("success_hold_ms")) / 1000.0)
+        except Exception:
+            return 0.35
+
+    def should_refine_llm(confidence: str, raw_text: str) -> bool:
+        if not settings.get("use_intelligence"):
+            return False
+
+        # Extra safety: skip LLM on short utterances (most common place for unintended "translation").
+        try:
+            min_audio_s = float(settings.get("llm_refine_min_audio_s"))
+            audio_s = float(getattr(transcriber, "last_stats", {}).get("audio_seconds", 0.0))
+            if audio_s and audio_s < min_audio_s:
+                return False
+        except Exception:
+            pass
+
+        try:
+            min_words = int(settings.get("llm_refine_min_words"))
+            if min_words > 0:
+                wc = len([w for w in (raw_text or "").strip().split() if w])
+                if wc < min_words:
+                    return False
+        except Exception:
+            pass
+
+        want = str(settings.get("llm_refine_min_confidence") or "high").lower()
+        rank = {"high": 3, "medium": 2, "low": 1, "silence": 0, "unknown": 0}
+        return rank.get(confidence, 0) >= rank.get(want, 3)
     
     def pipeline_worker():
         nonlocal stop_processing_flag
@@ -163,7 +195,7 @@
                      if raw_text:
                          # log(f"Raw ({time.time()-start_process:.2f}s): {raw_text}", "debug")
                          
-                         if settings.get("use_intelligence"):
+                         if should_refine_llm(getattr(transcriber, "last_confidence", "unknown"), raw_text):
                              final_text = intelligence.refine_text(raw_text)
                          else:
                              final_text = raw_text # Raw Mode
@@ -172,7 +204,7 @@
                          
                          injector.type_text(final_text)
                          ui_queue.put("SUCCESS")
-                         time.sleep(1.0)
+                         time.sleep(get_success_hold_s())
                      else:
                          pass 
                  except Exception as e:
@@ -187,6 +219,8 @@
         def _job():
             if processing_lock.acquire(blocking=False):
                 try:
+                    if stop_processing_flag:
+                        return
                     ui_queue.put("LISTENING")
                     audio_data = audio.listen_single_segment()
                     if len(audio_data) > 0:
@@ -196,11 +230,14 @@
                         
                         raw_text = transcriber.transcribe(audio_data, language=lang_code)
                         if raw_text:
-                            final_text = intelligence.refine_text(raw_text)
-                        else:
-                            final_text = raw_text # Raw Mode
+                            if should_refine_llm(getattr(transcriber, "last_confidence", "unknown"), raw_text):
+                                final_text = intelligence.refine_text(raw_text)
+                            else:
+                                final_text = raw_text
+
+                            injector.type_text(final_text)
                             ui_queue.put("SUCCESS")
-                            time.sleep(1)
+                            time.sleep(get_success_hold_s())
                 except Exception:
                     pass
                 finally:
@@ -237,6 +274,48 @@
     
     threading.Thread(target=start_hotkey, daemon=True).start()
 
+    def start_ptt_key_listener():
+        # Optional single-key PTT binding from Settings (e.g., `, space).
+        ignore_keys = {"ctrl", "shift", "alt", "meta", "cmd", "win"}
+
+        def normalize_key(k):
+            try:
+                # KeyCode -> character (e.g. '`')
+                if hasattr(k, "char") and k.char:
+                    return str(k.char).lower()
+            except Exception:
+                pass
+
+            try:
+                # Special keys (space, enter, etc.)
+                if isinstance(k, keyboard.Key):
+                    name = str(k).replace("Key.", "").lower()
+                    return name
+            except Exception:
+                pass
+
+            return None
+
+        def on_press(k):
+            try:
+                if settings.get("mode") != "push_to_talk":
+                    return
+                wanted = str(settings.get("push_to_talk_key") or "").lower()
+                if not wanted or wanted in ignore_keys:
+                    return
+                if normalize_key(k) == wanted:
+                    trigger_ptt_pass()
+            except Exception:
+                pass
+
+        try:
+            with keyboard.Listener(on_press=on_press) as listener:
+                listener.join()
+        except Exception:
+            pass
+
+    threading.Thread(target=start_ptt_key_listener, daemon=True).start()
+
     def on_quit():
         stop_processing_flag = True
         if audio: audio.stop_recording()
--- a/ui/overlay.py
+++ b/ui/overlay.py
@@ -1,15 +1,69 @@
 import sys
 import random
+import math
 from PyQt6.QtWidgets import QApplication, QWidget, QMenu
 from PyQt6.QtCore import Qt, QTimer, pyqtSignal, QPoint
-from PyQt6.QtGui import QPainter, QColor, QFont, QAction, QCursor
-
-class MatrixRainWidget(QWidget):
+from PyQt6.QtGui import QPainter, QColor, QFont, QAction, QCursor, QImage
+from core.settings import manager as settings
+
+class OverlayBaseWidget(QWidget):
     def __init__(self):
         super().__init__()
+        self.state = "IDLE"
+        self.locked = True
+        self.drag_pos = None
+
         # Window Flags: Frameless, On Top, Tool
-        self.setWindowFlags(Qt.WindowType.FramelessWindowHint | Qt.WindowType.WindowStaysOnTopHint | Qt.WindowType.Tool)
+        self.setWindowFlags(
+            Qt.WindowType.FramelessWindowHint
+            | Qt.WindowType.WindowStaysOnTopHint
+            | Qt.WindowType.Tool
+        )
         self.setAttribute(Qt.WidgetAttribute.WA_TranslucentBackground)
+
+    def set_state(self, state):
+        self.state = state
+        self.update()
+
+    # --- Mouse Interaction ---
+    def mousePressEvent(self, event):
+        if event.button() == Qt.MouseButton.LeftButton:
+            if not self.locked:
+                self.drag_pos = event.globalPosition().toPoint() - self.frameGeometry().topLeft()
+
+    def mouseMoveEvent(self, event):
+        if event.buttons() == Qt.MouseButton.LeftButton and not self.locked and self.drag_pos:
+            self.move(event.globalPosition().toPoint() - self.drag_pos)
+
+    def contextMenuEvent(self, event):
+        menu = QMenu(self)
+
+        settings_action = QAction("Settings", self)
+        settings_action.triggered.connect(self.open_settings)
+        menu.addAction(settings_action)
+
+        lock_action = QAction("Lock Position" if not self.locked else "Unlock Position", self)
+        lock_action.triggered.connect(self.toggle_lock)
+        menu.addAction(lock_action)
+
+        quit_action = QAction("Quit", self)
+        quit_action.triggered.connect(QApplication.instance().quit)
+        menu.addAction(quit_action)
+
+        menu.exec(event.globalPos())
+
+    def open_settings(self):
+        if hasattr(self, "on_settings_click"):
+            self.on_settings_click()
+
+    def toggle_lock(self):
+        self.locked = not self.locked
+        self.update()
+
+
+class MatrixRainWidget(OverlayBaseWidget):
+    def __init__(self):
+        super().__init__()
         
         # Geometry: Start Bottom Right
         screen = QApplication.primaryScreen().geometry()
@@ -24,10 +78,6 @@
         
         self.setGeometry(x, y, self.width_, self.height_)
         
-        self.state = "IDLE" 
-        self.locked = True
-        self.drag_pos = None
-
         # Matrix Rain Config
         self.font_size = 10 # Dense
         self.columns = int(self.width_ / self.font_size)
@@ -41,10 +91,6 @@
         self.timer.start(50) 
         
         self.show()
-
-    def set_state(self, state):
-        self.state = state
-        self.update()
 
     def animate(self):
         # Update drops
@@ -120,46 +166,277 @@
                 
                 painter.drawText(x, int(char_y), text)
 
-    # --- Mouse Interaction ---
-    def mousePressEvent(self, event):
-        if event.button() == Qt.MouseButton.LeftButton:
-            if not self.locked:
-                self.drag_pos = event.globalPosition().toPoint() - self.frameGeometry().topLeft()
+
+class DotWidget(OverlayBaseWidget):
+    def __init__(self):
+        super().__init__()
+
+        screen = QApplication.primaryScreen().geometry()
+        self.size_ = 22
+        x = screen.width() - 60
+        y = screen.height() - 80
+        self.setGeometry(x, y, self.size_, self.size_)
+        self.show()
+
+    def paintEvent(self, event):
+        painter = QPainter(self)
+        painter.setRenderHint(QPainter.RenderHint.Antialiasing, True)
+
+        if self.state == "IDLE":
+            color = QColor(0, 255, 70, 110)
+        elif self.state == "LISTENING":
+            color = QColor(255, 60, 60, 220)
+        elif self.state == "PROCESSING":
+            color = QColor(70, 170, 255, 220)
+        elif self.state == "SUCCESS":
+            color = QColor(255, 255, 255, 220)
+        else:
+            color = QColor(0, 255, 70, 110)
+
+        # Subtle glow
+        painter.setBrush(QColor(color.red(), color.green(), color.blue(), min(70, color.alpha())))
+        painter.setPen(Qt.PenStyle.NoPen)
+        painter.drawEllipse(1, 1, self.size_ - 2, self.size_ - 2)
+
+        painter.setBrush(color)
+        painter.drawEllipse(5, 5, self.size_ - 10, self.size_ - 10)
+
+        if not self.locked:
+            painter.setPen(QColor(255, 255, 0, 200))
+            painter.setBrush(Qt.BrushStyle.NoBrush)
+            painter.drawEllipse(0, 0, self.size_ - 1, self.size_ - 1)
+
+
+class SauronEyeWidget(OverlayBaseWidget):
+    def __init__(self):
+        super().__init__()
+
+        screen = QApplication.primaryScreen().geometry()
+        self.width_ = 220
+        self.height_ = 120
+        x = screen.width() - 300
+        y = screen.height() - 220
+        self.setGeometry(x, y, self.width_, self.height_)
+
+        self._phase = 0.0
+        self._spin = 0.0
+
+        # Procedural flame (Doom-fire style) buffer, rendered to a small QImage and scaled up.
+        self._fire_w = 120
+        self._fire_h = 64
+        self._fire = [0] * (self._fire_w * self._fire_h)  # intensity 0..255
+        self._fire_img = QImage(self._fire_w, self._fire_h, QImage.Format.Format_RGBA8888)
+        self._fire_palette = self._build_fire_palette()
+        self._fire_flicker = 0.0
+        self._fire_seed = 0.0
+
+        self.timer = QTimer(self)
+        self.timer.timeout.connect(self.animate)
+        try:
+            self.timer.start(int(settings.get("sauron_fire_fps_ms")))
+        except Exception:
+            self.timer.start(40)
+
+        self.show()
+
+    def animate(self):
+        self._phase += 0.06
+        self._spin += 1.6 if self.state == "PROCESSING" else 0.3
+        self._fire_flicker += 0.10 if self.state in {"LISTENING", "PROCESSING"} else 0.06
+        self._fire_seed += 0.02
+        self._update_fire()
+        self.update()
+
+    @staticmethod
+    def _lerp(a: float, b: float, t: float) -> float:
+        return a + (b - a) * t
+
+    def _build_fire_palette(self):
+        # Palette ramps from transparent black -> deep red -> orange -> yellow-white.
+        pal = []
+        for i in range(256):
+            t = i / 255.0
+            if t < 0.30:
+                r = int(self._lerp(0, 120, t / 0.30))
+                g = int(self._lerp(0, 10, t / 0.30))
+                b = 0
+            elif t < 0.70:
+                tt = (t - 0.30) / 0.40
+                r = int(self._lerp(120, 255, tt))
+                g = int(self._lerp(10, 120, tt))
+                b = int(self._lerp(0, 20, tt))
             else:
-                # pass click through? (Difficult in PyQt without extensive Win32 API)
-                pass
-        
-    def mouseMoveEvent(self, event):
-        if event.buttons() == Qt.MouseButton.LeftButton and not self.locked and self.drag_pos:
-            self.move(event.globalPosition().toPoint() - self.drag_pos)
-
-    def contextMenuEvent(self, event):
-        menu = QMenu(self)
-        
-        settings_action = QAction("Settings", self)
-        settings_action.triggered.connect(self.open_settings)
-        menu.addAction(settings_action)
-
-        lock_action = QAction("Lock Position" if not self.locked else "Unlock Position", self)
-        lock_action.triggered.connect(self.toggle_lock)
-        menu.addAction(lock_action)
-        
-        quit_action = QAction("Quit", self)
-        quit_action.triggered.connect(QApplication.instance().quit)
-        menu.addAction(quit_action)
-        
-        menu.exec(event.globalPos())
-
-    def open_settings(self):
-        # We need to signal the main thread or pass the engine
-        # Since this runs in the UI process, we can emit a signal or call a callback if passed.
-        # Ideally, main.py should pass a callback.
-        if hasattr(self, 'on_settings_click'):
-            self.on_settings_click()
-
-    def toggle_lock(self):
-        self.locked = not self.locked
-        self.update()
+                tt = (t - 0.70) / 0.30
+                r = 255
+                g = int(self._lerp(120, 240, tt))
+                b = int(self._lerp(20, 120, tt))
+
+            # Alpha: low values stay faint; high values glow.
+            a = int(255 * (t ** 0.85))
+            pal.append((r, g, b, a))
+        return pal
+
+    def _state_fire_params(self):
+        # Returns (base_intensity, decay_base, decay_rand, wind_strength, opacity)
+        if self.state == "IDLE":
+            return 135, 14, 24, 1, 0.35
+        if self.state == "LISTENING":
+            return 255, 10, 18, 2, 0.75
+        if self.state == "PROCESSING":
+            return 210, 12, 22, 3, 0.60
+        if self.state == "SUCCESS":
+            return 175, 12, 20, 1, 0.50
+        return 135, 14, 24, 1, 0.35
+
+    def _update_fire(self):
+        w, h = self._fire_w, self._fire_h
+        base_intensity, decay_base, decay_rand, wind_strength, _ = self._state_fire_params()
+
+        # Seed bottom line with flickery intensity.
+        pulse = 0.6 + 0.4 * math.sin(self._fire_flicker)
+        for x in range(w):
+            jitter = random.randint(-35, 35)
+            v = int(base_intensity * pulse) + jitter
+            self._fire[(h - 1) * w + x] = max(0, min(255, v))
+
+        # Propagate upward.
+        # Classic doom-fire: each cell takes from below and decays, with small horizontal drift (wind).
+        # Iterate from bottom-2 up to 0.
+        wind = int(wind_strength * math.sin(self._fire_seed * 6.0))
+        for y in range(h - 2, -1, -1):
+            row = y * w
+            below = (y + 1) * w
+            for x in range(w):
+                src = below + x
+                decay = decay_base + random.randint(0, decay_rand)
+                v = self._fire[src]
+                nv = v - int(decay * 0.6)
+                if nv < 0:
+                    nv = 0
+
+                # Drift slightly left/right with randomness + wind.
+                drift = random.randint(-1, 1) + wind
+                dx = x + drift
+                if dx < 0:
+                    dx = 0
+                elif dx >= w:
+                    dx = w - 1
+                self._fire[row + dx] = nv
+
+        # Render to QImage
+        ptr = self._fire_img.bits()
+        ptr.setsize(w * h * 4)
+        buf = memoryview(ptr)
+        pal = self._fire_palette
+
+        # Slight vertical fade (top is more transparent)
+        for y in range(h):
+            fade = y / max(1, h - 1)
+            fade_a = int(255 * (fade ** 1.6))
+            for x in range(w):
+                v = self._fire[y * w + x]
+                r, g, b, a = pal[v]
+                a = (a * fade_a) // 255
+                i = (y * w + x) * 4
+                buf[i + 0] = r
+                buf[i + 1] = g
+                buf[i + 2] = b
+                buf[i + 3] = a
+
+    def paintEvent(self, event):
+        painter = QPainter(self)
+        painter.setRenderHint(QPainter.RenderHint.Antialiasing, True)
+
+        # State-driven intensity
+        if self.state == "IDLE":
+            glow = 60
+            iris = QColor(255, 120, 0, 170)
+            pupil = QColor(0, 0, 0, 220)
+        elif self.state == "LISTENING":
+            glow = 170
+            iris = QColor(255, 40, 0, 230)
+            pupil = QColor(0, 0, 0, 235)
+        elif self.state == "PROCESSING":
+            glow = 140
+            iris = QColor(255, 160, 40, 220)
+            pupil = QColor(0, 0, 0, 235)
+        elif self.state == "SUCCESS":
+            glow = 120
+            iris = QColor(255, 220, 120, 210)
+            pupil = QColor(0, 0, 0, 235)
+        else:
+            glow = 60
+            iris = QColor(255, 120, 0, 170)
+            pupil = QColor(0, 0, 0, 220)
+
+        # Background
+        bg_alpha = 18 if self.state == "IDLE" else 90
+        if not self.locked:
+            bg_alpha = 70
+        painter.fillRect(self.rect(), QColor(0, 0, 0, bg_alpha))
+
+        cx = self.width_ // 2
+        cy = self.height_ // 2
+
+        # Flames (behind the eye)
+        _, _, _, _, flame_opacity = self._state_fire_params()
+        painter.save()
+        painter.setOpacity(flame_opacity)
+        painter.setCompositionMode(QPainter.CompositionMode.CompositionMode_Screen)
+        # Draw flames slightly above the eye center, then a faint reflection below.
+        flame_w = int(self.width_ * 1.05)
+        flame_h = int(self.height_ * 1.10)
+        painter.drawImage(int(cx - flame_w // 2), int(cy - flame_h // 2 - 18), self._fire_img.scaled(flame_w, flame_h, Qt.AspectRatioMode.IgnoreAspectRatio, Qt.TransformationMode.SmoothTransformation))
+        painter.setOpacity(flame_opacity * 0.25)
+        painter.drawImage(int(cx - flame_w // 2), int(cy - flame_h // 2 + 28), self._fire_img.mirrored(False, True).scaled(flame_w, int(flame_h * 0.55), Qt.AspectRatioMode.IgnoreAspectRatio, Qt.TransformationMode.SmoothTransformation))
+        painter.restore()
+
+        # Outer glow (eye aura)
+        pulse = 0.5 + 0.5 * math.sin(self._phase)
+        aura = QColor(255, 60, 0, int(glow * (0.55 + 0.45 * pulse)))
+        painter.setPen(Qt.PenStyle.NoPen)
+        painter.setBrush(aura)
+        painter.drawEllipse(int(cx - 92), int(cy - 42), 184, 84)
+
+        # Sclera (dark eye white)
+        painter.setBrush(QColor(10, 10, 10, 200))
+        painter.drawEllipse(int(cx - 84), int(cy - 36), 168, 72)
+
+        # Iris
+        painter.setBrush(iris)
+        painter.drawEllipse(int(cx - 46), int(cy - 28), 92, 56)
+
+        # Pupil (vertical slit)
+        slit_w = 10 + int(4 * math.sin(self._phase * 1.7))
+        painter.setBrush(pupil)
+        painter.drawRoundedRect(int(cx - slit_w // 2), int(cy - 24), slit_w, 48, 6, 6)
+
+        # Fiery ring details
+        ring_alpha = int(120 + 80 * pulse)
+        painter.setBrush(Qt.BrushStyle.NoBrush)
+        painter.setPen(QColor(255, 200, 60, ring_alpha))
+        painter.drawEllipse(int(cx - 52), int(cy - 32), 104, 64)
+
+        # Processing spinner arc
+        if self.state == "PROCESSING":
+            painter.setPen(QColor(255, 255, 255, 160))
+            r_w, r_h = 120, 76
+            painter.drawArc(int(cx - r_w // 2), int(cy - r_h // 2), r_w, r_h, int(self._spin * 16), int(110 * 16))
+
+        # Unlock outline
+        if not self.locked:
+            painter.setPen(QColor(255, 255, 0, 200))
+            painter.setBrush(Qt.BrushStyle.NoBrush)
+            painter.drawRoundedRect(0, 0, self.width_ - 1, self.height_ - 1, 10, 10)
+
+
+def create_overlay_widget(skin: str) -> OverlayBaseWidget:
+    s = (skin or "").strip().lower()
+    if s == "dot":
+        return DotWidget()
+    if s == "sauron_eye":
+        return SauronEyeWidget()
+    return MatrixRainWidget()
 
 def run_overlay(state_queue, on_settings_click=None, app=None):
     if app is None:
@@ -169,14 +446,39 @@
     # (MatrixWindow produces 'Qt.Tool' flag which is ignored by default quit logic)
     app.setQuitOnLastWindowClosed(False)
         
-    widget = MatrixRainWidget()
+    widget = create_overlay_widget(settings.get("overlay_skin"))
     widget.on_settings_click = on_settings_click
+    last_skin = str(settings.get("overlay_skin") or "")
     
     # Check queue for updates
     timer = QTimer()
     def check_queue():
-        if not state_queue.empty():
+        nonlocal widget, last_skin
+
+        # Hot-swap skins if changed.
+        current_skin = str(settings.get("overlay_skin") or "")
+        if current_skin != last_skin:
+            pos = widget.pos()
+            locked = getattr(widget, "locked", True)
+            state = getattr(widget, "state", "IDLE")
+
+            try:
+                widget.close()
+            except Exception:
+                pass
+
+            widget = create_overlay_widget(current_skin)
+            widget.on_settings_click = on_settings_click
+            widget.move(pos)
+            widget.locked = locked
+            widget.set_state(state)
+            last_skin = current_skin
+
+        # Drain to the latest state to avoid UI lag when updates come quickly.
+        state = None
+        while not state_queue.empty():
             state = state_queue.get()
+        if state is not None:
             widget.set_state(state)
     
     timer.timeout.connect(check_queue)
