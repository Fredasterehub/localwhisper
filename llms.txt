# llms.txt - LocalWhisper
# Machine-readable instructions for AI assistants
# Format: YAML blocks for structured data, markdown for context

## METADATA
```yaml
name: LocalWhisper
description: Local voice-to-text for Windows & macOS. Works everywhere including terminals.
repo: https://github.com/Fredasterehub/localwhisper
license: MIT
version: 1.0
```

## REQUIREMENTS
```yaml
python: ">=3.10"
platforms:
  windows:
    gpu: NVIDIA with CUDA
    vram_minimum: 6GB
    os: Windows 10+
  macos:
    gpu: Apple Silicon (M1/M2/M3)
    os: macOS 12+
optional:
  - ollama  # for grammar correction
```

## INSTALL_WINDOWS
```yaml
steps:
  - cmd: git clone https://github.com/Fredasterehub/localwhisper.git
  - cmd: cd localwhisper
  - cmd: install.bat
  - cmd: run.bat
```

## INSTALL_MACOS
```yaml
status: coming_soon
note: Native Apple Silicon version ready, upload pending
```

## VRAM_MODEL_SELECTION
```yaml
# Adjust WHISPER_MODEL_SIZE in config.py based on available VRAM
options:
  - vram: ">=24GB"
    model: large-v3-turbo
    compute_type: float16
  - vram: "8-16GB"
    model: large-v3-turbo
    compute_type: float16
  - vram: "6-8GB"
    model: medium
    compute_type: float16
  - vram: "<6GB"
    model: small
    compute_type: int8
```

## USAGE
```yaml
hotkey: Ctrl+Alt+W
workflow:
  1: Focus any text field (terminal, browser, IDE, etc.)
  2: Press hotkey - overlay appears
  3: Speak - overlay turns red while recording
  4: Stop speaking - overlay turns blue, text is typed
```

## CONFIGURATION
```yaml
files:
  config.py:
    purpose: Main settings (model, hotkey, Ollama URL)
    key_variables:
      - WHISPER_MODEL_SIZE  # Model size (see VRAM_MODEL_SELECTION)
      - HOTKEY              # Activation hotkey
      - OLLAMA_URL          # Must use 127.0.0.1, not localhost
      - OLLAMA_MODEL        # Default: gemma3:1b
  core/settings.py:
    purpose: Advanced parameters
  user_settings.json:
    purpose: Persisted user preferences (auto-generated)
```

## TROUBLESHOOTING
```yaml
issues:
  - symptom: CUDA not found
    solution: Install NVIDIA CUDA Toolkit 11.8+

  - symptom: No audio detected
    solution: Open Settings > Input Device, verify green level bar moves when speaking

  - symptom: Text not appearing in target window
    solution: Run application as Administrator

  - symptom: Ollama connection errors
    solution: Start Ollama first with `ollama serve`

  - symptom: Slow response (>1s latency)
    solution: Verify config.py uses OLLAMA_URL="http://127.0.0.1:11434/api/generate" (not localhost)

logs: logs/session.log
```

## PERFORMANCE
```yaml
benchmarks:
  hardware: RTX 4090 + i9-14900K
  whisper_inference: 0.23s
  grammar_correction: 0.35s
  total_latency: 0.6s
  vram_usage: 6GB
```

## TECH_STACK
```yaml
components:
  transcription: faster-whisper (large-v3-turbo)
  vad: silero-vad v4
  grammar: ollama + gemma3:1b
  gui: PyQt6
  tui: textual
  audio: sounddevice
```

## RELATED_FILES
```yaml
for_claude_code: CLAUDE.md  # Detailed dev instructions
for_humans: README.md       # Full documentation
```
